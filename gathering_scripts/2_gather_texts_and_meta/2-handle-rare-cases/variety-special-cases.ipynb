{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebook for experiments in gathering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import requests\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_name = 'variety'\n",
    "df_path = f\"../1-most-common-page-structure/mod_df/mod_{media_name}_df.csv\"\n",
    "errors_path = f\"../1-most-common-page-structure/errors/{media_name}_errors.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size = 187222\n",
      "err_df size = 97504\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://variety.com/2015/film/columns/the-best...</td>\n",
       "      <td>cannot save data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://variety.com/2016/film/columns/herschel...</td>\n",
       "      <td>cannot save data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://variety.com/2015/film/columns/midnight...</td>\n",
       "      <td>cannot save data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://variety.com/2015/film/columns/richard-...</td>\n",
       "      <td>cannot save data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://variety.com/2018/film/columns/oscars-b...</td>\n",
       "      <td>cannot save data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url            reason\n",
       "0  https://variety.com/2015/film/columns/the-best...  cannot save data\n",
       "1  https://variety.com/2016/film/columns/herschel...  cannot save data\n",
       "2  https://variety.com/2015/film/columns/midnight...  cannot save data\n",
       "3  https://variety.com/2015/film/columns/richard-...  cannot save data\n",
       "4  https://variety.com/2018/film/columns/oscars-b...  cannot save data"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(df_path)\n",
    "df = df.drop_duplicates()\n",
    "err_df = pd.read_csv(errors_path)\n",
    "err_df = err_df.drop_duplicates()\n",
    "print(\"df size =\", len(df))\n",
    "print(\"err_df size =\", len(err_df))\n",
    "err_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(media_name)\n",
    "except:\n",
    "    print(\"cannot create dir\")\n",
    "\n",
    "#df_article_class_names = [\"article__body js-fitvids-content\", \"longform__body-primary\"]\n",
    "df_meta_names = ['content_type', 'topics', 'title', 'author', 'published_at', 'tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "def get_html(url):\n",
    "    fake_headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    response = requests.get(url, headers=fake_headers)\n",
    "    return response.text\n",
    "\n",
    "def get_html_test():\n",
    "    test_url = df.iloc[0]['article_url']\n",
    "    print(test_url)\n",
    "    print(get_html(test_url))\n",
    "    \n",
    "#get_html_test()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_type': 'Article',\n",
       " 'topics': 'Global, Markets & Festivals, News',\n",
       " 'title': 'Ventana Sur: Portugal’s Pepe Rapazote Toplines Argentine Leonardo Brzezicki’s ‘Almost in Love’ (EXCLUSIVE)',\n",
       " 'author': 'Anna Marie de la Fuente',\n",
       " 'published_at': '2018-12-12 03:06:26',\n",
       " 'tags': 'Ventana Sur'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "'''\n",
    "def get_article_text(html_text):\n",
    "    soup = BeautifulSoup(html_text, \"lxml\")\n",
    "    for class_name_str in df_article_class_names:\n",
    "        mydivs = soup.find(\"div\", {\"class\": class_name_str})\n",
    "        if mydivs != None:\n",
    "            text = ''\n",
    "            for p in mydivs.find_all(\"p\"):\n",
    "                text += p.text + ' '\n",
    "            return text\n",
    "    return None\n",
    "'''\n",
    "\n",
    "def get_article_text(html_text):\n",
    "    soup = BeautifulSoup(html_text, \"lxml\")\n",
    "    text = soup.find(\"meta\", {\"name\": 'body'})['content']\n",
    "    return text\n",
    "\n",
    "def get_article_meta(html_text, meta_names):\n",
    "    ret = {}\n",
    "    soup = BeautifulSoup(html_text, \"lxml\")\n",
    "    for name in meta_names:\n",
    "        for meta in soup.find_all(\"meta\", {\"name\": name}):\n",
    "            if name not in ret:\n",
    "                ret[name] = meta['content']\n",
    "            elif ret[name] != meta['content']:\n",
    "                ret[name] = ret[name] + ', ' + meta['content']\n",
    "    return ret\n",
    "\n",
    "def get_article_text_test():    \n",
    "    test_url = df.iloc[0]['article_url']\n",
    "    html_text = get_html(test_url)\n",
    "    return get_article_text(html_text)\n",
    "\n",
    "def get_article_meta_test():    \n",
    "    test_url = df.iloc[0]['article_url']    \n",
    "    html_text = get_html(test_url)\n",
    "    return get_article_meta(html_text, df_meta_names)\n",
    "\n",
    "#get_article_meta_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article_urls(df, err_df, meta_names):    \n",
    "    modified_df = df        \n",
    "    cannot_parsed = {} # {url : reason}\n",
    "    with tqdm(desc=\"rows\", total=len(err_df)) as pbar_outer:\n",
    "        for row in err_df.itertuples():\n",
    "            url = getattr(row, 'url')\n",
    "            # part 0: get html text\n",
    "            html_text = None\n",
    "            try:\n",
    "                html_text = get_html(url)\n",
    "            except:\n",
    "                cannot_parsed[url] = 'cannot get html'\n",
    "                pbar_outer.update(1)\n",
    "                continue                \n",
    "            # part 1: get article text\n",
    "            article_text = None\n",
    "            try:\n",
    "                article_text = get_article_text(html_text)                          \n",
    "            except:\n",
    "                cannot_parsed[url] = 'cannot get text'\n",
    "                pbar_outer.update(1)\n",
    "                continue\n",
    "            # part 2: get article meta\n",
    "            article_meta = None\n",
    "            try:\n",
    "                article_meta = get_article_meta(html_text, meta_names)\n",
    "            except:\n",
    "                cannot_parsed[url] = 'cannot get meta'\n",
    "                pbar_outer.update(1)\n",
    "                continue\n",
    "            # part 3: save data\n",
    "            try:\n",
    "                df_row = df.loc[df.article_url == url].to_dict()\n",
    "                index = list(df_row['text_path'].keys())[0]\n",
    "                text_path = list(df_row['text_path'].values())[0]\n",
    "                for name in meta_names:\n",
    "                    if name in article_meta:\n",
    "                        modified_df.at[index,name] = article_meta[name]        \n",
    "                file = open(text_path,'w')\n",
    "                file.write(article_text)  \n",
    "                file.close()\n",
    "            except:\n",
    "                cannot_parsed[url] = 'cannot save data'\n",
    "            pbar_outer.update(1)\n",
    "    return (modified_df, cannot_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rows:   0%|          | 1/97504 [00:00<23:39:08,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_df, errors = parse_article_urls(df, err_df, df_meta_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(errors)>0:\n",
    "    file = open(f'{media_name}_errors.csv', \"w\")\n",
    "    f = csv.writer(file)\n",
    "    f.writerow([\"url\", \"reason\"])\n",
    "    for key, value in errors.items():\n",
    "      f.writerow([key, value])\n",
    "    file.close()\n",
    "    \n",
    "#mod_df.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_df = mod_df.drop(df.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_df.to_csv(f\"full_{media_name}_df.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101880"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df = pd.read_csv(f'./full_df/full_hollywood_reporter_df.csv')\n",
    "err_df = pd.read_csv('hollywood_reporter_errors.csv')\n",
    "print(len(df), len(err_df))\n",
    "\n",
    "for row in err_df.itertuples():\n",
    "    url = getattr(row, 'url')\n",
    "    df_row = df.loc[df.article_url == url].to_dict()\n",
    "    index = list(df_row['text_path'].keys())[0]\n",
    "    df = df.drop(index=index)\n",
    "len(df)    \n",
    "\n",
    "print(len(df), len(err_df))\n",
    "\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "df.to_csv(f\"full_{media_name}_df.csv\", index=True)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
